"""
Module `train_domain_select.py`

Ce script entra√Æne un mod√®le de classification supervis√©e pour pr√©dire le *domaine*
associ√© √† un document √† partir de ses vecteurs TF-IDF.  
Il combine plusieurs √©tapes de s√©lection et r√©duction de dimensions, puis entra√Æne
un classifieur logistique avec pond√©ration des classes.

Fonctionnalit√©s principales :
-----------------------------
- S√©lection de features via test du chi¬≤ (`SelectKBest`)
- R√©duction dimensionnelle via SVD (`TruncatedSVD`)
- Calcul de similarit√©s cosinus entre documents et centro√Ødes de classes
- Normalisation des features finales
- Entra√Ænement d‚Äôun mod√®le de r√©gression logistique pond√©r√©e
- Sauvegarde de tous les objets n√©cessaires √† l‚Äôinf√©rence : s√©lecteur, SVD, scaler,
  centro√Ødes, classifieur, encodeur et liste des colonnes non nulles

Fichiers d‚Äôentr√©e :
-------------------
- `data/processed/tfidf_vectors.csv` : vecteurs TF-IDF
- `data/labeled.csv` : labels de domaines

Fichiers de sortie :
--------------------
- `models/domain_selector.joblib` : s√©lecteur de features chi¬≤
- `models/domain_svd.joblib` : mod√®le de r√©duction dimensionnelle SVD
- `models/domain_scaler.joblib` : scaler pour standardisation
- `models/domain_centroids.joblib` : centro√Ødes calcul√©s pour chaque classe
- `models/domain_clf.joblib` : mod√®le de r√©gression logistique entra√Æn√©
- `models/domain_label_encoder.joblib` : encodeur de labels
- `models/domain_nonzero_columns.joblib` : liste des colonnes non nulles utilis√©es
"""

import os
import joblib
import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics.pairwise import cosine_similarity
from commite_github import commit_file_to_github

# --- D√©finition des chemins de base ---
BASE = os.path.dirname(__file__)
VECT_CSV = os.path.join(BASE, "..", "..", "..", "data", "processed", "tfidf_vectors.csv")
LAB_CSV  = os.path.join(BASE, "..", "..", "..", "data", "labeled.csv")
MODELS_DIR = os.path.join(BASE, "..", "..", "..", "models")
os.makedirs(MODELS_DIR, exist_ok=True)

# --- Hyperparam√®tres globaux ---
K_SELECT = 3000        # Nombre de features √† garder apr√®s s√©lection chi¬≤
SVD_COMPONENTS = 150   # Nombre de composantes SVD (r√©duction dimensionnelle)
CLASS_WEIGHTED = True  # Active la pond√©ration automatique des classes pour LogisticRegression


def _canon_label(s):
    """
    Normalise une √©tiquette textuelle (label) en la convertissant en minuscule et en retirant les espaces.

    Param√®tres
    ----------
    s : str ou NaN
        Valeur brute du label.

    Retour
    ------
    str
        Label nettoy√© en minuscules, ou "unknown" si la valeur est manquante.
    """
    return str(s).strip().lower() if pd.notna(s) else "unknown"


def train_domain():
    """
    Entra√Æne un mod√®le de classification du domaine √† partir de vecteurs TF-IDF.

    √âtapes principales :
    --------------------
    1. Chargement et fusion des donn√©es de vecteurs et labels.
    2. Nettoyage, conversion et suppression des features nulles.
    3. S√©lection de K meilleures features via test du chi¬≤.
    4. Calcul de centro√Ødes moyens par classe et de similarit√©s cosinus.
    5. R√©duction dimensionnelle via SVD.
    6. Normalisation des features et concat√©nation des similarit√©s.
    7. Entra√Ænement du mod√®le de r√©gression logistique (avec `class_weight='balanced'`).
    8. Sauvegarde de tous les objets n√©cessaires √† la pr√©diction future.

    Exceptions
    ----------
    RuntimeError
        Si aucune correspondance entre vecteurs et labels n‚Äôest trouv√©e.
    """

    # --- Chargement des donn√©es ---
    df_vec = pd.read_csv(VECT_CSV, sep=";")
    df_lab = pd.read_csv(LAB_CSV, sep=";")

    # Conversion explicite des identifiants en cha√Æne
    df_vec["doc"] = df_vec["doc"].astype(str)
    df_lab["doc"] = df_lab["doc"].astype(str)

    # --- Fusion des donn√©es et pr√©paration des labels ---
    df = pd.merge(df_vec, df_lab, on="doc", how="inner")
    if df.shape[0] == 0:
        raise RuntimeError("Aucune correspondance entre vecteurs et labels")
    
    # Canonisation des labels de domaine
    df["domain_y"] = df["domain_y"].apply(_canon_label)

    # --- Extraction des features ---
    X_df = df.drop(columns=["doc", "domain_y"], errors="ignore")

    # Conversion en num√©rique et remplacement des NaN
    X_df = X_df.apply(pd.to_numeric, errors="coerce").fillna(0)

    # Suppression des colonnes totalement nulles
    col_sums = X_df.sum(axis=0)
    nonzero_cols = col_sums[col_sums > 0].index.tolist()
    X_df = X_df[nonzero_cols]
    print("Features after zero-drop:", X_df.shape[1])

    # --- Pr√©paration des labels ---
    y_raw = df["domain_y"]
    le = LabelEncoder()
    y = le.fit_transform(y_raw)

    # --- Conversion en matrice sparse ---
    X_sp = csr_matrix(X_df.values)

    # --- S√©lection de features via chi¬≤ ---
    k = min(K_SELECT, X_sp.shape[1] - 1)
    selector = SelectKBest(chi2, k=k)
    selector.fit(X_sp, y)
    X_sel = selector.transform(X_sp)

    # --- Calcul des centro√Ødes par classe ---
    classes = np.unique(y)
    centroids = []
    for c in classes:
        rows = (y == c)
        if rows.sum() == 0:
            # Classe vide : vecteur nul
            centroids.append(np.zeros(X_sel.shape[1], dtype=float))
        else:
            s = X_sel[rows].sum(axis=0)
            centroid = np.asarray(s).ravel() / float(max(1, rows.sum()))
            centroids.append(centroid)
    centroids = np.vstack(centroids)

    # --- Similarit√© cosinus entre chaque document et les centro√Ødes ---
    cos_sim = cosine_similarity(X_sel, centroids)

    # --- R√©duction dimensionnelle via SVD ---
    n_comp = min(SVD_COMPONENTS, X_sel.shape[1] - 1)
    svd = TruncatedSVD(n_components=n_comp, random_state=42)
    X_red = svd.fit_transform(X_sel)

    # --- Standardisation des features r√©duites ---
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_red)

    # --- Combinaison des features finales (SVD + similarit√©s cosinus) ---
    X_final = np.hstack([X_scaled, cos_sim])

    # --- Entra√Ænement du classifieur ---
    clf = LogisticRegression(
        class_weight="balanced" if CLASS_WEIGHTED else None,
        solver="saga",
        max_iter=2000
    )
    clf.fit(X_final, y)

    # --- Sauvegarde de tous les objets n√©cessaires √† la pr√©diction ---
    files_to_commit = [
    os.path.join(MODELS_DIR, "domain_selector.joblib"),
    os.path.join(MODELS_DIR, "domain_svd.joblib"),
    os.path.join(MODELS_DIR, "domain_scaler.joblib"),
    os.path.join(MODELS_DIR, "domain_centroids.joblib"),
    os.path.join(MODELS_DIR, "domain_clf.joblib"),
    os.path.join(MODELS_DIR, "domain_label_encoder.joblib"),
    os.path.join(MODELS_DIR, "domain_nonzero_columns.joblib")
    ]
    
    for file_path in files_to_commit:
        commit_file_to_github(
            local_file_path=file_path,
            repo_path=file_path,  # conserve le m√™me chemin dans le repo GitHub
            commit_message=f"Update {os.path.basename(file_path)}"
        )
        print(f"üöÄ {os.path.basename(file_path)} commit√© sur GitHub avec succ√®s !")


# --- Point d‚Äôentr√©e principal ---
if __name__ == "__main__":
    train_domain()
